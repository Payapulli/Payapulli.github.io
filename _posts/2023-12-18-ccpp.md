---
title: "Linear Regression, KNN Regression: Combined Cycle Power Plant Data"
date: 2023-12-18
categories:
  - Data Science
---

This project delves into a detailed dataset from a Combined Cycle Power Plant over six years (2006-2011), where the main goal is to predict the net hourly electrical energy output (EP). The dataset features include hourly average ambient variables: Temperature (T), Ambient Pressure (AP), Relative Humidity (RH), and Exhaust Vacuum (V)...

![Alt text for image](/assets/images/Combined-Cycle-Power-Plant.jpeg)

<!--more-->

## Project Overview
The project begins with downloading and exploring the dataset, examining the number of rows and columns, and understanding what they represent. This step includes making scatterplots of all variables and observing the relationships between independent and dependent variables.

## Data Analysis and Insights
Statistical analysis is conducted to find the mean, median, range, quartiles, and interquartile ranges of each variable. This is summarized in a comprehensive table for an easy overview of the data characteristics.

## Regression Models
The core of the project involves:
- Developing simple linear regression models for each predictor and assessing their significance in predicting the plant's output.
- Constructing a multiple regression model using all predictors and identifying the most impactful ones.
- A comparative study between univariate and multiple regression models is presented visually.

## Advanced Analysis
The exploration extends to:
- Examining nonlinear associations between predictors and response using polynomial regression models.
- Investigating interactions between different predictors and their impact on the response.
- A thorough analysis of these models is presented, including the training and testing Mean Squared Errors (MSEs).

## KNN Regression
The project also includes KNN regression analysis, comparing both normalized and raw features to determine the best fit. The analysis of train and test errors provides valuable insights into the model's performance.

## Conclusions

My linear regression with quadratic and interaction terms (and statistically insignificant terms dropped) is my best performing linear regressor. This has a test MSE of 17.8. Both my raw and normalized KNN regressors outperform this, with MSE's of 15.9 and 14.0 respectively.

So clearly, KNN Regression outperforms Linear Regression for this dataset. I believe this is due to the following reasons:

- **Nonlinearity**: There is some degree of nonlinearity in the data. This is made clear by fact that the complex model performs better than the basic model, as it includes quadratic terms which have p-values that are effectively at zero. Whilst this is an improvement on the basic model, it is entirely possible that there are even more complex, nonlinear patterns within the data that these quadratic terms are not able to capture. Evidence to support this idea is found in part (f), where we found that the introduction of cubic terms for some of the predictors was statistically significant. The KNN Regression model is naturally better suited to capture such nonlinearity.
- **Interaction**: On a similar note we saw in part (g) that some of the predictors in our dataset have statistically significant interactions. And while these pairwise interactions were included in the complex model which improved its MSE, there may also be higher order interactions that are not captured. Such interactions are captured inherently by the KNN model as it measure the multi-dimensional distance across these features.
- **Noise**: From our analysis in part (c), there is not a lot of noise in the data. Only a very small amount of points have been flagged as "outliers" in my influence plot, and these data points have been removed from the dataset as well. I believe this bodes well for the KNN, as it allows it essentially increases its performance for smaller values of k which allows it to be more precise (at the risk of overfitting). Our value of k, that minimizes the test error is 5, which supports this theory.
- **Assumptions**: Linear Regression makes further assumptions about the data such as constant variance of errors, independence of error, and that errors are normally distributed. It may be true that such assumptions do not hold for this dataset which causes the KNN Regressor to outperform it.

It is important to note that the difference in MSE for the KNN Regression and Linear Regression is not massive, and the Linear Regression model does come with advantages such as better interpretability and scales better with added features.

[View Project on GitHub](https://github.com/Payapulli/power-plant-prediction) |
[View Dataset](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)