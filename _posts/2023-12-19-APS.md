---
title: "Random Forests, Boosting: Analyzing APS Failure Data with Advanced Machine Learning "
date: 2023-12-19
categories:
  - Data Science
---

This project revolves around the analysis of the APS Failure dataset from Scania Trucks, focusing on implementing tree-based methods for classification. The dataset, rich in attributes and challenges, offers an excellent opportunity to delve into techniques like Random Forest, XGBoost, and addressing class imbalances.


![Alt text for image](/assets/images/scania-trucks.jpeg)

<!--more-->

## Data Preparation and Exploration
### Dataset Acquisition and Preprocessing
- **Data Download**: Retrieving the APS Failure data, comprising 60,000 rows with 171 columns.
- **Handling Missing Values**: Researching and applying techniques to manage significant missing values in the dataset.

### Feature Analysis
- **Coefficient of Variation**: Calculating the CV for each of the 170 features.
- **Correlation Matrix**: Creating a matrix to understand the interdependencies between features.
- **Feature Visualization**: Selecting a subset of features with the highest CV for scatter and box plot visualization, aiding in assessing feature significance.

### Imbalance Assessment
- **Data Balance Analysis**: Determining the proportion of positive and negative instances to evaluate dataset balance.

## Random Forest Classification
### Initial Model Training
- **Model Training**: Developing a random forest model without addressing class imbalance.
- **Model Evaluation**: Assessing the model using confusion matrix, ROC, AUC, and misclassification rates for both training and test sets. Calculating the Out of Bag error estimate and comparing it with the test error.

### Addressing Class Imbalance
- **Imbalance Handling**: Applying Synthetic Minority Over-sampling Technique (SMOTE) to address class imbalance.
- **Imbalance Compensation**: Re-training the random forest with class imbalance compensation and comparing the results to the initial model.

### Results

The test error and training error goes up when we compensate for class imbalance which is to be expected, the nature of the data we are making predictions on is heavily imbalanced in favor of the negative class so when we deliberately force our model to balance its weights more towards the positive class the training and test error will naturally increase (especially the training error).

However, the advantage here is that our model has become much better at classifying the positive samples where it is now correctly classifying 363 (12 missclassifications) of the positive samples whereas before it was only able correctly classify 268 (missclassifying 107) of the positive samples. This comes at a cost to its ability to classify the negative samples however, which leads to our overall test error increasing. But if we are particularly interested in our ability to classify positive samples then this can be considered beneficial.

## Advanced Tree-Based Techniques
### XGBoost and Model Trees
- **Model Tree Training**: Implementing an XGBoost model tree, using L1-penalized logistic regression at each node. 
- **Error Estimation**: Employing different cross-validation methods (5-fold, 10-fold, and leave-one-out) to estimate model error and comparing it with test error.
- **Model Metrics**: Reporting the Confusion Matrix, ROC, and AUC for both training and test sets.

### SMOTE for Class Imbalance
- **SMOTE Preprocessing**: Applying Synthetic Minority Over-sampling Technique (SMOTE) to address class imbalance.
- **Model Re-training**: Training the XGBoost model with L1-penalized logistic regression on the pre-processed data and repeating the error estimation and comparison.

### Results

Similar to part d, using SMOTE on our XGBoost model increases our overall training and testing error (making the model worse in terms of overall performance). However, we can see from our confusion matrices that the model becomes better at classifying positive samples, at the cost of reduced accuracy on the negative samples which leads to the increased training/test error.

[View Project on GitHub](https://github.com/Payapulli/aps-failure-prediction) |
[View Dataset](https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks)