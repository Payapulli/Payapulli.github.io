---
title: "Decision Trees and Advanced Regression Techniques on Crime data"
date: 2023-12-19
categories:
  - Data Science
---

In this project, I delve into the realms of decision trees and advanced regression techniques, applying these methods to two distinct datasets: the Acute Inflammations dataset and the Communities and Crime dataset. The project is structured into two main sections, each focusing on different aspects of machine learning.

![Alt text for image](/assets/images/crime-data.png)

<!--more-->

## Decision Trees as Interpretable Models
### Dataset Acquisition and Tree Construction
- **Data Download**: Fetching the Acute Inflamations data from its official repository.
- **Decision Tree Modeling**: Building a decision tree on the entire dataset and visualizing it.

### Rule Extraction and Pruning
- **Rule Conversion**: Translating the decision rules of the tree into a set of IF-THEN rules for better interpretability.
- **Cost-Complexity Pruning**: Implementing pruning techniques to find a minimal decision tree that maintains high interpretability.

## The LASSO and Boosting for Regression
### Data Handling and Analysis
- **Data Retrieval**: Downloading the Communities and Crime dataset.
- **Data Preparation**: Handling missing values and disregarding nonpredictive features.
- **Data Exploration**: Creating a correlation matrix to understand feature relationships.
- **Coefficient of Variation Calculation**: Computing the Coefficient of Variation (CV) for each feature.

### Feature Selection and Visualization
- **Feature Shortlisting**: Selecting a subset of features with the highest CV.
- **Data Visualization**: Generating scatter plots and box plots for the shortlisted features to assess their significance.

### Regression Models and Evaluation
- **Linear Regression**: Fitting a linear model using least squares and reporting the test error.
- **Ridge Regression**: Applying ridge regression with λ chosen by cross-validation and reporting the test error.
- **LASSO Regression**: Conducting LASSO regression with λ chosen by cross-validation and comparing the test error between models with standard and non-standardized features.
- **PCR Model**: Fitting a Principal Component Regression (PCR) model with the number of components chosen by cross-validation.

### Advanced Regression with Boosting
- **L1 Penalized Gradient Boosting Tree**: Utilizing XGBoost to fit an L1 penalized gradient boosting tree, determining α through cross-validation.

### Conclusion
All of our models have a similar performance at around 0.017-0.018 test error with our Ridge Regression model coming out as the slight favorite with a test error of 0.017732.

[View Jupyter Notebook](https://nbviewer.org/github/Payapulli/Payapulli.github.io/blob/main/jupyter-notebooks/crime-lasso-boosting.ipynb) |
[View Project on GitHub](https://github.com/Payapulli/communities-crime-prediction) |
[View Inflammations Dataset](https://archive.ics.uci.edu/ml/datasets/Acute+Inflammations) |
[View Crime Dataset](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime)