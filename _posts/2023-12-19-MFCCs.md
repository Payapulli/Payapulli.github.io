---
title: "K-Means Clustering: Multi-Class and Multi-Label Classification on Anuran Calls"
date: 2023-12-19
categories:
  - Data Science
---

This project is a deep dive into advanced machine learning techniques using the Anuran Calls (MFCCs) Data Set. The focus is on multi-class and multi-label classification using Support Vector Machines (SVMs) and exploring K-Means clustering for a multi-class, multi-label dataset.

![Alt text for image](/assets/images/anuran-calls.jpeg)

<!--more-->

## Part 1: Multi-Class and Multi-Label Classification Using SVMs
### Data Preparation and Setup
- **Data Download**: Fetching the Anuran Calls dataset and splitting it into a 70% training set.
- **Problem Understanding**: Each instance has three labels (Families, Genus, Species), presenting a multi-class and multi-label classification problem.

### Binary Relevance Approach
- **Evaluation Metrics**: Researching and applying exact match and hamming score/loss methods for evaluating the classifiers.
- **Gaussian Kernel SVMs**: Training a SVM for each label using Gaussian kernels and one-vs-all approach, optimizing SVM penalty and Gaussian kernel width via 10-fold cross-validation.

### L1-Penalized SVMs
- **Implementation and Standardization**: Implementing L1-penalized SVMs with standardized attributes, optimizing the SVM penalty using 10-fold cross-validation.

### Addressing Class Imbalance
- **Class Imbalance Remediation**: Applying SMOTE to address class imbalance and repeating the L1-penalized SVM training.
- **Comparative Analysis**: Reporting conclusions about the classifiers' performance.

### Additional Practices
- **Classifier Chain Method**: Studying and applying the Classifier Chain method to the classification problem.
- **Multi-Label Metrics Analysis**: Researching and computing confusion matrices, precision, recall, ROC, and AUC for the trained classifiers.

### Results
The original SVM model with no L1 penalty and no SMOTE resampling performs the best, even in terms of recall which we can typically expect SMOTE to improve for imbalanced datasets.

Between the L1 Penalty model and the L1 Penalty with SMOTE model it is less clear. The L1 penalty model performs better in terms of exact match, hamming score and precision but if we are particularly interested in reducing False Negatives (Recall) and F1 score then we might choose the SMOTE model instead.

However, assuming that we are not especially interested in recall/F1 then I will rank the models as below:

SVM
L1 Penalty
SMOTE

## Part 2: K-Means Clustering on a Multi-Class and Multi-Label Data Set
### Monte-Carlo Simulation
- **Clustering Execution**: Performing K-means clustering on the entire dataset 50 times, choosing k automatically via CH, Gap Statistics, scree plots, Silhouettes, or another method.
- **Majority Label Determination**: Identifying the majority family, genus, and species in each cluster by reading the true labels.

### Hamming Distance Analysis
- **Label Triplet Assignment**: Assigning a majority label triplet to each cluster.
- **Distance Calculation**: Computing the average Hamming distance, score, and loss between true labels and cluster-assigned labels, reporting the average and standard deviation of these metrics across 50 simulations.


### Results
In the unsupervised setting, the K-means clustering model performs reasonably well on this multi-label, multi-classification problem with a hamming loss of 0.217.

[View Jupyter Notebook](https://nbviewer.org/github/Payapulli/Payapulli.github.io/blob/main/jupyter-notebooks/MFCC-SVM-KMeans.ipynb) | 
[View Project on GitHub](https://github.com/Payapulli/anuran-calls-classification) |
[View Dataset](https://archive.ics.uci.edu/ml/datasets/Anuran+Calls+%28MFCCs%29)