<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-12-19T22:37:24-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Joshua Payapulli</title><subtitle>Portfolio</subtitle><author><name>Joshua Payapulli</name></author><entry><title type="html">Transfer Learning, CNN + MLP: Identification of Frost in Martian HiRISE Images</title><link href="http://localhost:4000/data%20science/mars/" rel="alternate" type="text/html" title="Transfer Learning, CNN + MLP: Identification of Frost in Martian HiRISE Images" /><published>2023-12-19T00:00:00-08:00</published><updated>2023-12-19T00:00:00-08:00</updated><id>http://localhost:4000/data%20science/mars</id><content type="html" xml:base="http://localhost:4000/data%20science/mars/"><![CDATA[<p>In this project, I tackle the challenge of identifying frost in Martian terrain images using advanced machine learning techniques. The task involves analyzing a unique dataset of Martian terrain images, implementing and comparing a Convolutional Neural Network + Multi-Layer Perceptron trained from scratch vs Transfer Learning with pre-trained networks (EfficientNetB0, ResNet50, and VGG16).</p>

<p><img src="/assets/images/mars-frost.jpeg" alt="Alt text for image" /></p>

<!--more-->

<h2 id="data-exploration-and-pre-processing">Data Exploration and Pre-processing</h2>
<h3 id="dataset-overview">Dataset Overview</h3>
<ul>
  <li><strong>Data Retrieval</strong>: Downloading the Martian HiRISE Images dataset, which consists of 214 subframes and a total of 119920 tiles, each labeled as ‘frost’ or ‘background.’</li>
</ul>

<h3 id="data-organization-and-preparation">Data Organization and Preparation</h3>
<ul>
  <li><strong>Data Splitting</strong>: Utilizing provided files to divide the dataset into training, testing, and validation sets.</li>
  <li><strong>Image Processing</strong>: Implementing an image augmentation pipeline to crop, zoom, rotate, and translate the images to make our model more robust to different orientations/scaling.</li>
</ul>

<h2 id="training-cnn--mlp">Training CNN + MLP</h2>
<h3 id="model-architecture-and-training">Model Architecture and Training</h3>
<ul>
  <li><strong>CNN Implementation</strong>: Building a three-layer CNN followed by a dense layer. Incorporating ReLU activations, softmax function, batch normalization, dropout (30%), L2 regularization, and ADAM optimizer.</li>
  <li><strong>Training Process</strong>: Training the model for 20 epochs with early stopping based on validation set performance. Plotting training and validation errors vs. epochs.</li>
</ul>

<h3 id="model-evaluation">Model Evaluation</h3>
<ul>
  <li><strong>Performance Metrics</strong>: Reporting Precision, Recall, and F1 score for the CNN + MLP model.</li>
</ul>

<h2 id="transfer-learning-with-pre-trained-models">Transfer Learning with Pre-trained Models</h2>
<h3 id="implementing-transfer-learning">Implementing Transfer Learning</h3>
<ul>
  <li><strong>Pre-trained Models</strong>: Using EfficientNetB0, ResNet50, and VGG16 as feature extractors. Training only the last fully connected layer and freezing the previous layers.</li>
  <li><strong>Model Training</strong>: Repeating image augmentation processes and training each model for 10 epochs. Implementing early stopping and plotting errors vs. epochs.</li>
</ul>

<h3 id="model-evaluation-1">Model Evaluation</h3>
<ul>
  <li><strong>Performance Metrics</strong>: Reporting Precision, Recall, and F1 score for each transfer learning model.</li>
</ul>

<h3 id="conclusion">Conclusion</h3>
<p>It is immediately apparent that this dataset is very challenging to learn from even for Pre-trained models such as ResNet50 that usually perform very well on image classification problems.</p>

<p>The CNN + MLP model did not really seem to learn from the data as it has a training accuracy 0.57 and a test accuracy of 0.49. The Transfer Learning models (specifically the ResNet and VGG models) however are able to learn from the data but are likely overfitting. ResNet has a training accuracy of 0.78 and VGG has a training accuracy of 0.92, with a test accuracy of 0.55, 0.53 respectively.</p>

<p>As such, we can say that the Transfer learning models (specifically ResNet and VGG) outperform the CNN + MLP model as they are at least able to learn from the data, despite definitely being overfit, they also have a slightly higher test accuracy. This makes sense as our test data (images of Mars) is probably very different to the data that these Transfer Learning models learned on, so these models are probably learning some pattern from these data that does not generalize well to new unseen data of this type (Martian terrain).</p>

<p><a href="https://nbviewer.org/github/Payapulli/Payapulli.github.io/blob/main/jupyter-notebooks/Vertebral-Column-Datset-KNN.ipynb">View Jupyter Notebook</a> | 
<a href="URL_to_your_GitHub_repository">View Project on GitHub</a> |
<a href="https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant">View Dataset</a></p>]]></content><author><name>Joshua Payapulli</name></author><category term="Data Science" /><summary type="html"><![CDATA[In this project, I tackle the challenge of identifying frost in Martian terrain images using advanced machine learning techniques. The task involves analyzing a unique dataset of Martian terrain images, implementing and comparing a Convolutional Neural Network + Multi-Layer Perceptron trained from scratch vs Transfer Learning with pre-trained networks (EfficientNetB0, ResNet50, and VGG16).]]></summary></entry><entry><title type="html">Advanced Machine Learning: Multi-Class and Multi-Label Classification &amp;amp; K-Means Clustering</title><link href="http://localhost:4000/data%20science/MFCCs/" rel="alternate" type="text/html" title="Advanced Machine Learning: Multi-Class and Multi-Label Classification &amp;amp; K-Means Clustering" /><published>2023-12-19T00:00:00-08:00</published><updated>2023-12-19T00:00:00-08:00</updated><id>http://localhost:4000/data%20science/MFCCs</id><content type="html" xml:base="http://localhost:4000/data%20science/MFCCs/"><![CDATA[<p>This project is a deep dive into advanced machine learning techniques using the Anuran Calls (MFCCs) Data Set. The focus is on multi-class and multi-label classification using Support Vector Machines (SVMs) and exploring K-Means clustering for a multi-class, multi-label dataset.</p>

<p><img src="/assets/images/anuran-calls.jpeg" alt="Alt text for image" /></p>

<!--more-->

<h2 id="part-1-multi-class-and-multi-label-classification-using-svms">Part 1: Multi-Class and Multi-Label Classification Using SVMs</h2>
<h3 id="data-preparation-and-setup">Data Preparation and Setup</h3>
<ul>
  <li><strong>Data Download</strong>: Fetching the Anuran Calls dataset and splitting it into a 70% training set.</li>
  <li><strong>Problem Understanding</strong>: Each instance has three labels (Families, Genus, Species), presenting a multi-class and multi-label classification problem.</li>
</ul>

<h3 id="binary-relevance-approach">Binary Relevance Approach</h3>
<ul>
  <li><strong>Evaluation Metrics</strong>: Researching and applying exact match and hamming score/loss methods for evaluating the classifiers.</li>
  <li><strong>Gaussian Kernel SVMs</strong>: Training a SVM for each label using Gaussian kernels and one-vs-all approach, optimizing SVM penalty and Gaussian kernel width via 10-fold cross-validation.</li>
</ul>

<h3 id="l1-penalized-svms">L1-Penalized SVMs</h3>
<ul>
  <li><strong>Implementation and Standardization</strong>: Implementing L1-penalized SVMs with standardized attributes, optimizing the SVM penalty using 10-fold cross-validation.</li>
</ul>

<h3 id="addressing-class-imbalance">Addressing Class Imbalance</h3>
<ul>
  <li><strong>Class Imbalance Remediation</strong>: Applying SMOTE to address class imbalance and repeating the L1-penalized SVM training.</li>
  <li><strong>Comparative Analysis</strong>: Reporting conclusions about the classifiers’ performance.</li>
</ul>

<h3 id="additional-practices">Additional Practices</h3>
<ul>
  <li><strong>Classifier Chain Method</strong>: Studying and applying the Classifier Chain method to the classification problem.</li>
  <li><strong>Multi-Label Metrics Analysis</strong>: Researching and computing confusion matrices, precision, recall, ROC, and AUC for the trained classifiers.</li>
</ul>

<h2 id="part-2-k-means-clustering-on-a-multi-class-and-multi-label-data-set">Part 2: K-Means Clustering on a Multi-Class and Multi-Label Data Set</h2>
<h3 id="monte-carlo-simulation">Monte-Carlo Simulation</h3>
<ul>
  <li><strong>Clustering Execution</strong>: Performing K-means clustering on the entire dataset 50 times, choosing k automatically via CH, Gap Statistics, scree plots, Silhouettes, or another method.</li>
  <li><strong>Majority Label Determination</strong>: Identifying the majority family, genus, and species in each cluster by reading the true labels.</li>
</ul>

<h3 id="hamming-distance-analysis">Hamming Distance Analysis</h3>
<ul>
  <li><strong>Label Triplet Assignment</strong>: Assigning a majority label triplet to each cluster.</li>
  <li><strong>Distance Calculation</strong>: Computing the average Hamming distance, score, and loss between true labels and cluster-assigned labels, reporting the average and standard deviation of these metrics across 50 simulations.</li>
</ul>

<p><a href="URL_to_your_GitHub_repository">View Project on GitHub</a></p>]]></content><author><name>Joshua Payapulli</name></author><category term="Data Science" /><summary type="html"><![CDATA[This project is a deep dive into advanced machine learning techniques using the Anuran Calls (MFCCs) Data Set. The focus is on multi-class and multi-label classification using Support Vector Machines (SVMs) and exploring K-Means clustering for a multi-class, multi-label dataset.]]></summary></entry><entry><title type="html">Tree-Based Methods: Analyzing APS Failure Data with Advanced Machine Learning Techniques</title><link href="http://localhost:4000/data%20science/APS/" rel="alternate" type="text/html" title="Tree-Based Methods: Analyzing APS Failure Data with Advanced Machine Learning Techniques" /><published>2023-12-19T00:00:00-08:00</published><updated>2023-12-19T00:00:00-08:00</updated><id>http://localhost:4000/data%20science/APS</id><content type="html" xml:base="http://localhost:4000/data%20science/APS/"><![CDATA[<p>This project revolves around the analysis of the APS Failure dataset from Scania Trucks, focusing on implementing tree-based methods for classification. The dataset, rich in attributes and challenges, offers an excellent opportunity to delve into techniques like Random Forest, XGBoost, and addressing class imbalances.</p>

<p><img src="/assets/images/scania-trucks.jpeg" alt="Alt text for image" /></p>

<!--more-->

<h2 id="data-preparation-and-exploration">Data Preparation and Exploration</h2>
<h3 id="dataset-acquisition-and-preprocessing">Dataset Acquisition and Preprocessing</h3>
<ul>
  <li><strong>Data Download</strong>: Retrieving the APS Failure data, comprising 60,000 rows with 171 columns.</li>
  <li><strong>Handling Missing Values</strong>: Researching and applying techniques to manage significant missing values in the dataset.</li>
</ul>

<h3 id="feature-analysis">Feature Analysis</h3>
<ul>
  <li><strong>Coefficient of Variation</strong>: Calculating the CV for each of the 170 features.</li>
  <li><strong>Correlation Matrix</strong>: Creating a matrix to understand the interdependencies between features.</li>
  <li><strong>Feature Visualization</strong>: Selecting a subset of features with the highest CV for scatter and box plot visualization, aiding in assessing feature significance.</li>
</ul>

<h3 id="imbalance-assessment">Imbalance Assessment</h3>
<ul>
  <li><strong>Data Balance Analysis</strong>: Determining the proportion of positive and negative instances to evaluate dataset balance.</li>
</ul>

<h2 id="random-forest-classification">Random Forest Classification</h2>
<h3 id="initial-model-training">Initial Model Training</h3>
<ul>
  <li><strong>Model Training</strong>: Developing a random forest model without addressing class imbalance.</li>
  <li><strong>Model Evaluation</strong>: Assessing the model using confusion matrix, ROC, AUC, and misclassification rates for both training and test sets. Calculating the Out of Bag error estimate and comparing it with the test error.</li>
</ul>

<h3 id="addressing-class-imbalance">Addressing Class Imbalance</h3>
<ul>
  <li><strong>Research on Imbalance Handling</strong>: Investigating how class imbalance is managed in random forests.</li>
  <li><strong>Imbalance Compensation</strong>: Re-training the random forest with class imbalance compensation and comparing the results to the initial model.</li>
</ul>

<h2 id="advanced-tree-based-techniques">Advanced Tree-Based Techniques</h2>
<h3 id="xgboost-and-model-trees">XGBoost and Model Trees</h3>
<ul>
  <li><strong>Model Tree Training</strong>: Implementing an XGBoost model tree, using L1-penalized logistic regression at each node.</li>
  <li><strong>Error Estimation</strong>: Employing different cross-validation methods (5-fold, 10-fold, and leave-one-out) to estimate model error and comparing it with test error.</li>
  <li><strong>Model Metrics</strong>: Reporting the Confusion Matrix, ROC, and AUC for both training and test sets.</li>
</ul>

<h3 id="smote-for-class-imbalance">SMOTE for Class Imbalance</h3>
<ul>
  <li><strong>SMOTE Preprocessing</strong>: Applying Synthetic Minority Over-sampling Technique (SMOTE) to address class imbalance.</li>
  <li><strong>Model Re-training</strong>: Training the XGBoost model with L1-penalized logistic regression on the pre-processed data and repeating the error estimation and comparison.</li>
</ul>

<p><a href="https://nbviewer.org/github/Payapulli/Payapulli.github.io/blob/main/jupyter-notebooks/Vertebral-Column-Datset-KNN.ipynb">View Jupyter Notebook</a> | 
<a href="URL_to_your_GitHub_repository">View Project on GitHub</a> |
<a href="https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant">View Dataset</a></p>]]></content><author><name>Joshua Payapulli</name></author><category term="Data Science" /><summary type="html"><![CDATA[This project revolves around the analysis of the APS Failure dataset from Scania Trucks, focusing on implementing tree-based methods for classification. The dataset, rich in attributes and challenges, offers an excellent opportunity to delve into techniques like Random Forest, XGBoost, and addressing class imbalances.]]></summary></entry><entry><title type="html">Logistic Regression: Human Activity/Gesture detction on time series data</title><link href="http://localhost:4000/data%20science/AReM/" rel="alternate" type="text/html" title="Logistic Regression: Human Activity/Gesture detction on time series data" /><published>2023-12-19T00:00:00-08:00</published><updated>2023-12-19T00:00:00-08:00</updated><id>http://localhost:4000/data%20science/AReM</id><content type="html" xml:base="http://localhost:4000/data%20science/AReM/"><![CDATA[<p>In this project, I explore the intricate task of classifying human activities based on time series data from the AReM dataset. This dataset, derived from a Wireless Sensor Network, includes detailed recordings of seven different types of human activities. Each activity is captured through six time series, presenting a rich dataset for analysis. The project is divided into two main parts: Feature Creation/Extraction and Binary/Multiclass Classification.</p>

<p><img src="/assets/images/human-activity.jpeg" alt="Alt text for image" /></p>

<!--more-->

<h3 id="data-preparation-and-segmentation">Data Preparation and Segmentation</h3>
<ul>
  <li><strong>Dataset Download</strong>: Acquiring the AReM dataset from its official source.</li>
  <li><strong>Training and Test Split</strong>: Strategically dividing the dataset into training and test sets based on predefined criteria.</li>
</ul>

<h3 id="feature-extraction">Feature Extraction</h3>
<ul>
  <li><strong>Time-Domain Feature Research</strong>: Identifying various time-domain features commonly used in time series classification.</li>
  <li><strong>Feature Extraction Process</strong>: Extracting key features like minimum, maximum, mean, median, standard deviation, first quartile, and third quartile from each time series.</li>
  <li><strong>Normalization/Standardization</strong>: Considering the option to normalize or standardize these features.</li>
  <li><strong>Statistical Analysis</strong>: Estimating the standard deviation of each feature and constructing a 90% bootstrap confidence interval.</li>
  <li><strong>Feature Selection</strong>: Selecting the most significant time-domain features based on analysis.</li>
</ul>

<h3 id="binary-classification-using-logistic-regression">Binary Classification Using Logistic Regression</h3>
<ul>
  <li><strong>Feature-based Visualization</strong>: Creating scatter plots of the selected features to distinguish between bending and other activities.</li>
  <li><strong>Time Series Segmentation for Analysis</strong>: Breaking each time series into two or more segments and examining the impact on feature representation.</li>
  <li><strong>Logistic Regression Modeling</strong>: Implementing logistic regression with various segmentations and feature sets, including p-value analysis for model parameters.</li>
  <li><strong>Model Evaluation</strong>: Assessing the model through confusion matrices, ROC curves, AUC, and logistic regression parameter significance.</li>
</ul>

<h3 id="binary-classification-using-l1-penalized-logistic-regression">Binary Classification Using L1-penalized Logistic Regression</h3>
<ul>
  <li><strong>Implementation of L1-penalized Logistic Regression</strong>: Utilizing L1 regularization in logistic regression and comparing it with variable selection using p-values.</li>
  <li><strong>Model Comparison and Analysis</strong>: Evaluating the effectiveness and ease of implementation between L1-penalized and standard logistic regression.</li>
</ul>

<h3 id="multiclass-classification">Multiclass Classification</h3>
<ul>
  <li><strong>L1-penalized Multinomial Regression</strong>: Implementing and testing an L1-penalized multinomial regression model for classifying all activities.</li>
  <li><strong>Naïve Bayes Classification</strong>: Comparing Gaussian and Multinomial Naïve Bayes classifiers for multiclass classification.</li>
</ul>

<p><a href="https://nbviewer.org/github/Payapulli/Payapulli.github.io/blob/main/jupyter-notebooks/Vertebral-Column-Datset-KNN.ipynb">View Jupyter Notebook</a> | 
<a href="URL_to_your_GitHub_repository">View Project on GitHub</a> |
<a href="https://archive.ics.uci.edu/dataset/366/activity+recognition+system+based+on+multisensor+data+fusion+arem">View Dataset</a></p>]]></content><author><name>Joshua Payapulli</name></author><category term="Data Science" /><summary type="html"><![CDATA[In this project, I explore the intricate task of classifying human activities based on time series data from the AReM dataset. This dataset, derived from a Wireless Sensor Network, includes detailed recordings of seven different types of human activities. Each activity is captured through six time series, presenting a rich dataset for analysis. The project is divided into two main parts: Feature Creation/Extraction and Binary/Multiclass Classification.]]></summary></entry><entry><title type="html">How to get better at chess: analyzing 1000 of my chess games.</title><link href="http://localhost:4000/data%20science/chess/" rel="alternate" type="text/html" title="How to get better at chess: analyzing 1000 of my chess games." /><published>2023-12-19T00:00:00-08:00</published><updated>2023-12-19T00:00:00-08:00</updated><id>http://localhost:4000/data%20science/chess</id><content type="html" xml:base="http://localhost:4000/data%20science/chess/"><![CDATA[<p>How Can Analyzing a History of Personal Chess Games Reveal General Trends, Strengths, and Weaknesses in Gameplay? This project embarks on an in-depth analysis of all chess games I have played over the past year. The objective is to unearth recurring themes and effective strategies, thereby elevating my game. Leveraging the data collected from 1000 of my games on Chess.com in the past year, this report seeks to extract practical insights, shaping my future tactics and approach in chess…</p>

<p>Evaluating one’s games individually is common practice in chess. Nowadays players use chess engines such as Stockfish and AlphaZero to highlight errors in a particular game. The approach this project takes is different, in that we are leveraging the power of Stockfish to find high-level insights, and common trends that can give us insight into the strengths and weaknesses of one’s game.</p>

<p><img src="/assets/images/chess/board.png" alt="Alt text for image" /></p>

<!--more-->

<h2 id="data-collection-and-cleaning">Data Collection and Cleaning:</h2>
<ol>
  <li>
    <p>The core data for this project was sourced from Chess.com, a leading online chess platform. Using their public API, I fetched game data from the year, which included a wide array of information such as move sequences (PGNs), player ratings, game outcomes, and timestamps. This data did not require much cleaning, rather just filtering out unnecessary/irrelevant information.</p>
  </li>
  <li>
    <p>I am doing web scraping on https://www.365chess.com/eco.php, to get a comprehensive list of all the names of chess openings based on their ECO abbreviation which is available from the Chess.com API. Using this, I am able to translate the ECO code from the Chess.com API into an understandable opening name. For example: A00 - Polish opening. Using this data did require quite a lot of cleaning, such as removing duplicate entries, removing unnecessary punctuation, quotation marks etc.</p>
  </li>
</ol>

<h3 id="in-depth-analysis">In-depth Analysis</h3>
<p>Let’s walk through the results of my analysis. A game of chess can be broken down into three distinctive parts; the opening, the middle game and the endgame. As such, I have followed this structure for my analysis of my previous chess games by also breaking it down into these three sections. We will find that analyzing each of these sections separately is conducive to producing distinct, actionable and illuminating insights into the strengths and weaknesses of my chess ability.</p>

<p>We will be referring to the term ‘evaluation’ a lot in this report. This refers to the advantage at a given position on the chess board as determined by ‘Stockfish’ - an extremely powerful chess engine. An evaluation of 0 means there is no advantage for black or white, the position is completely tied. An evaluation of 10,000 means white has won, and an evaluation of -10,000 means black has won. For the purposes of this report, in my ‘run_analysis.py’ file, I am multiplying the black score by -1 so that we can aggregate scores across all my games for black and white and come out with meaningful results. If we did not do this, the games I win as black and the games I win as white would cancel each other out and result, and vice versa for the games I lose as white and the games I lose as black.</p>

<h3 id="opening-middle-and-endgame-strongest-vs-weakest">Opening, middle and endgame. Strongest vs weakest.</h3>
<p>The first question I set out to answer was to find out which part of my game required the most work. I classify the ‘opening’ as the first 20 moves, ‘middle’ as the moves between 20-40, and the ‘end’ as all moves past 40. This serves as a reasonable cutoff between the three sections of the game.</p>

<p><img src="/assets/images/chess/game_phase_box_plot.jpg" alt="Alt text for image" /></p>

<p>In the opening phase, the evaluations are tightly grouped around a median close to zero, indicating that initial positions are typically balanced, reflecting standard opening theory. As the game transitions into the middle game, there’s a noticeable increase in the range of evaluations, with a median that shifts slightly positive, suggesting that during this phase, there’s a greater potential for one side to secure an advantage. Finally, in the endgame, while the median hovers around zero similar to the opening, the evaluations vary widely, demonstrating the decisive nature of this phase with pronounced advantages or pitfalls often influenced by material imbalances or pawn promotion scenarios.
At the grandmaster level, draws are commonplace, but the majority of the games I have played have resulted in either wins or losses. As such, we should expect the evaluation whether positive or negative to be fairly small in the opening, increase (become more positive or more negative) towards the middle game and then increase further in the endgame. So in conclusion, the average evaluation across these three sections do not immediately highlight any one part of my game that is stronger or weaker.
So, overall this does not reveal anything too insightful about where my strengths and weaknesses lie. It is clear however, that I am far from the grandmaster level for whom we would expect the distributions for the middle and endgame to be much more tightly centered around zero, as draws are much more common at this level.</p>

<h2 id="analyzing-my-opening-repertoire">Analyzing my opening repertoire.</h2>
<p>The opening is often referred to by chess players as ‘theory’, as these opening positions have all been intensely studied, to find which paths are optimal for each player. Strong chess players memorize this opening theory, so they can increase their chances to start the game in an advantageous position. The different paths/positions that a chess player likes to play in the beginning is often referred to as their ‘opening repertoire’.</p>

<p>The chess.com API provides access to the ‘ECO’ code which is a 3 character code to describe the opening that is played for a particular game. Using this data, and by scraping the translation table from https://www.365chess.com/eco.php to convert these ECO codes into their respective opening names I am able to produce the chart below, highlighting my 10 most frequently played openings.</p>

<p><img src="/assets/images/chess/frequent_openings.jpg" alt="Alt text for image" /></p>

<p>As we can see from the above chart, my most frequently played opening is the Caro-Kann defense and the Two knights defense.</p>

<p><img src="/assets/images/chess/frequent_openings_eval.jpg" alt="Alt text for image" /></p>

<p>In analyzing the average evaluations for the top 10 chess openings I frequently play, we can see some distinct trends that can inform my strategy. The Caro-Kann Defence, Pirc Defence and Old Indian defense tend to lead into less favorable positions on average, which may suggest a need to refine my approach or consider alternative openings when looking for a solid yet dynamic position.</p>

<p>Conversely, the Two Knights Defence, Giuoco Piano and the Philidor’s Defence seem to grant me more advantageous positions, which aligns with my experiences of achieving strong, assertive postures early in the game when playing these openings.</p>

<p>The Queen’s Pawn, and Italian game openings result in nearly balanced positions, indicating these are reliable choices for a solid game, though they may not confer significant early advantages. These openings are versatile and provide me with ample opportunities to outplay my opponent in the middlegame.</p>

<p>Moderate positive evaluations for the Sicilian Defence, and the Scandinavian Defence suggest that these openings align well with my play style, often leading to slightly better positions that I can capitalize on as the game progresses.</p>

<h3 id="average-evaluation-over-time">Average evaluation over time</h3>
<p>The visualization below was produced by aggregating the evaluation metric across each of my most frequently played openings, and each move number. This chart is extremely useful as it allows us to see where my opening game diverts from the theory for which are the optimal moves to play at each position.</p>

<p><img src="/assets/images/chess/avg_eval_by_move_no.jpg" alt="Alt text for image" /></p>

<p>The detailed line graph demonstrates the fluctuations in average evaluation across the first 20 moves for my top 10 most commonly played openings. Each line represents a different opening, and the trend of these lines provides insight into how these openings fare in terms of gaining or conceding an advantage during the early game.</p>

<p>Notably, the Philidor’s Defence, represented by the lime green line, shows a significant spike in evaluation after move 20, indicating a potential turning point where a substantial advantage was achieved. This suggests that when playing the Caro-Kann Defence, the positions I reach tend to be stable until a critical moment arises that decisively shifts the balance.</p>

<p>Other openings, like the Two Knights Defence, show more volatility in their evaluations. This could reflect the tactical nature of these openings, where each move can greatly impact the game’s outcome. The Queen’s Pawn shows a more steady trend, implying a more strategic approach where the evaluation does not swing wildly.</p>

<p>The convergence of lines around the zero mark until about move 10 suggests an even contest in the opening phase across different openings. However, as the game progresses, the diverging paths of the lines indicate that my understanding and execution in the middlegame could be the differentiator in outcomes.</p>

<p>Overall a clear takeaway from this chart is my lack of understanding of the theory for my weaker openings, such as the Pirc Defence or the Italian game, really starts to become evident at around move 13. Therefore, I should try to focus on studying the theory past this point.</p>

<p>Also, we can see that volatile/sharp openings like the Two Knights Defence seem to do well for me overall, so when learning new openings I should consider focusing my openings repertoire on playing sharp opening lines.</p>

<h2 id="middle-game-blunder-analysis">Middle game: Blunder Analysis</h2>
<p>As an intermediate player, one can make significant improvements in the middle game by reducing what is referred to as blunders. A blunder is defined by an error that allows your opponent to gain significant advantage in the game.</p>

<p>I have categorized blunders into two different types, tactical and strategic. A tactical blunder is where there has been significant loss in material, such as losing a rook or queen, with no compensation. A strategic blunder however is a bit more subtle, and refers to mainly positional errors. For example, moving your pawns in front of your bishop, blocking it off, putting a knight on the edge of the board, or having a ‘backwards’ pawn.</p>

<p>The way I differentiate between these two types of blunders is by comparing the drop in evaluation with the loss in material. If the evaluation has dropped further than 1000 centipawns, it is considered a blunder. Furthermore, if the material loss was greater than 3, (a pawn is worth 1, bishop is worth 3 etc.) then this blunder is categorized as tactical. Otherwise, we consider it a strategic blunder.</p>

<p><img src="/assets/images/chess/blunder_type_dist.jpg" alt="Alt text for image" /></p>

<p>The pie chart presented reveals a distribution of blunder types that I’ve made during the middle game phase of my chess matches. A significant majority, accounting for 77.1%, are classified as strategic blunders. These are errors that pertain to plans and long-term positional understanding, indicating that I might need to strengthen my overall game strategy, positional play, and perhaps reassess some of the middle-game plans I commonly employ.</p>

<p>On the other hand, tactical blunders, which involve direct calculation mistakes such as missing forks, pins, or discoveries, constitute a smaller portion, at 22.9%. While fewer in number, these mistakes are often the most immediately punishing, as they can directly lead to loss of material or other concrete disadvantages. This probably is due to the fact that I am an intermediate player, where we would expect a beginner to make mostly tactical errors. As such, to improve my play, it would be advisable to focus on studying strategic elements such as pawn structures, space control, and piece coordination.</p>

<h3 id="board-heatmap">Board Heatmap</h3>
<p>In the analysis process, each game was processed move by move. A heatmap matrix – an 8x8 array representing the chessboard – was updated for every piece on each square after every move. This tracking was facilitated by the update_board_heatmap function, which incremented matrix cells corresponding to the occupied squares.</p>

<p>The visualization of the heatmap illustrates the frequency of piece occupation on the board. The color intensity in each square correlates with the level of activity, providing a clear indication of the most and least frequented squares.</p>

<p><img src="/assets/images/chess/board_heatmap.jpg" alt="Alt text for image" /></p>

<p>My strategic emphasis on central control is evident, as the squares d4, e4, d5, and e5 exhibit a lot of activity. This is reflective of my gameplay style, which prioritizes dominating the center to facilitate tactical plays and improve piece mobility. The
heatmap validates my focus on these central squares and supports the fundamental chess principle that central control is crucial for successful gameplay.</p>

<p>The peripheral files, particularly the a and h files, show less activity, which is consistent with the general progression of a chess game where the flanks are less critical in the opening and middle phases. However, the increased activity in the corner squares suggests my consistent approach to rook mobilization and king safety, indicating a propensity for castling and rook lifts as part of my overall strategy.</p>

<p>Reflecting on this heatmap, it’s clear that my play style is centered around control and piece development, with a balanced yet assertive approach to the opening and middle game. Again, this is indicative of my progression as a player past the beginner level where these fundamental concepts are less ingrained in the player’s mind but also not at an advanced level where unorthodox strategies that may violate these principles are employed.</p>

<h3 id="strongest-and-weakest-endgames">Strongest and weakest endgames</h3>
<p>Similar to the opening, the amount of variability involved at the endgame is greatly reduced relative to the middle game. As such, there is again, well traversed theory for how a player should play optimally in the endgame. This is often referred to as ‘endgame technique’.</p>

<p>So, we can categorize endgames based on the pieces involved, and calculate the average evaluation for each type of endgame to gain a better understanding of where my ‘endgame technique’ needs to improve. My classification of these endgames is based on piece value, so the endgame is defined by the highest value piece on the board. For example, if I have a King, Rook, Bishop it would be classified as a ‘Rook endgame’ whereas if I had a King, Rook, Queen it would be classified as a ‘Queen endgame’. This is a slight oversimplification, but it allows for easy separation of endgame categories, and in the endgame the most valuable piece does often play the most important role. Therefore, this does not seem to be an overly reductive representation of the different types of endgames.</p>

<p><img src="/assets/images/chess/avg_eval_by_endgame.jpg" alt="Alt text for image" /></p>

<p>First off, we should note that the average evaluation here is positive for all the different types of endgames which may seem a bit misleading as we know that I have lost more games than I have won out of the 1000 games I selected. However, keep in mind that a lot of wins occur when the opponent resigns before we reach the endgame which is why these values are skewed in the positive direction. Nonetheless, this chart is still illustrative in determining which endgames are stronger and which are weaker.</p>

<p>We can see from this chart that the bishop and queen endgames are my strongest, and also that my knight endgame is the weakest. Pawn and rook endgames seem to be at more or less the same level.</p>

<p>Therefore, to improve my endgame technique, the first port of call should be to study endgames involving a knight and pawns.</p>

<h3 id="conclusion">Conclusion</h3>

<p>In conclusion, the analysis provided by these charts and data visualizations has given me valuable insights into my chess play style and performance across different phases of the game. From understanding the average evaluations in my most frequent openings to dissecting my endgame strategies, I now have a clearer picture of where my strengths lie and which areas require more focus and improvement.</p>

<p>In the future I would like to be able to fit a machine learning model to my chess games data to understand more complex ideas/patterns about my style of play. For example, whether I am a more tactical or positional player, or whether I am more effective with bishops, knights or rooks etc. Also, doing an analysis of my time management in game and which parts of my game deteriorate under time pressure would be very illustrative for me and my development as a player.</p>

<p><a href="https://nbviewer.org/github/Payapulli/Payapulli.github.io/blob/main/jupyter-notebooks/Vertebral-Column-Datset-KNN.ipynb">View Jupyter Notebook</a> | 
<a href="URL_to_your_GitHub_repository">View Project on GitHub</a></p>]]></content><author><name>Joshua Payapulli</name></author><category term="Data Science" /><summary type="html"><![CDATA[How Can Analyzing a History of Personal Chess Games Reveal General Trends, Strengths, and Weaknesses in Gameplay? This project embarks on an in-depth analysis of all chess games I have played over the past year. The objective is to unearth recurring themes and effective strategies, thereby elevating my game. Leveraging the data collected from 1000 of my games on Chess.com in the past year, this report seeks to extract practical insights, shaping my future tactics and approach in chess…]]></summary></entry><entry><title type="html">Exploring Machine Learning: Supervised, Semi-Supervised, and Unsupervised Learning</title><link href="http://localhost:4000/data%20science/breast-cancer/" rel="alternate" type="text/html" title="Exploring Machine Learning: Supervised, Semi-Supervised, and Unsupervised Learning" /><published>2023-12-19T00:00:00-08:00</published><updated>2023-12-19T00:00:00-08:00</updated><id>http://localhost:4000/data%20science/breast-cancer</id><content type="html" xml:base="http://localhost:4000/data%20science/breast-cancer/"><![CDATA[<p>This project embarks on an exploratory journey through different realms of machine learning: supervised, semi-supervised, and unsupervised learning, using the Breast Cancer Wisconsin (Diagnostic) Data Set and the Banknote Authentication Data Set. The focus is on implementing various classification and clustering techniques, and comparing their effectiveness through a series of Monte-Carlo simulations.</p>

<p><img src="/assets/images/breast-cancer.png" alt="Alt text for image" /></p>

<!--more-->

<h2 id="part-1-breast-cancer-wisconsin-diagnostic-data-set">Part 1: Breast Cancer Wisconsin (Diagnostic) Data Set</h2>
<h3 id="data-preparation-and-analysis">Data Preparation and Analysis</h3>
<ul>
  <li><strong>Data Download</strong>: Retrieving the dataset and dividing it into a training set (70%) and a test set (30%).</li>
</ul>

<h3 id="supervised-learning-with-svm">Supervised Learning with SVM</h3>
<ul>
  <li><strong>L1-Penalized SVM</strong>: Training an L1-penalized SVM and using 5-fold cross-validation for penalty parameter selection. Reporting average metrics and plotting the ROC for one of the runs.</li>
</ul>

<h3 id="semi-supervised-learning-self-training">Semi-Supervised Learning (Self-training)</h3>
<ul>
  <li><strong>Training with Partial Labels</strong>: Implementing a self-training approach with partially labeled data. Gradually adding unlabeled data to the training set based on their distance to the SVM decision boundary. Evaluating performance and reporting average metrics.</li>
</ul>

<h3 id="unsupervised-learning-with-k-means">Unsupervised Learning with K-Means</h3>
<ul>
  <li><strong>Cluster Analysis</strong>: Running k-means clustering on the dataset, assuming k=2. Analyzing cluster centers and predicting labels based on majority polling. Computing average accuracy, precision, recall, F1-score, AUC, and reporting the ROC and confusion matrix for one of the runs.</li>
</ul>

<h3 id="spectral-clustering">Spectral Clustering</h3>
<ul>
  <li><strong>Kernel-based Clustering</strong>: Implementing spectral clustering with an RBF kernel. Comparing the balance of clusters with the original dataset and using the fit-predict method for classification.</li>
</ul>

<h3 id="comparative-analysis">Comparative Analysis</h3>
<ul>
  <li><strong>Comparing Learning Approaches</strong>: Evaluating the performance differences between supervised, semi-supervised, and unsupervised learning methods.</li>
</ul>

<h2 id="part-2-active-learning-using-svms-on-banknote-authentication-data-set">Part 2: Active Learning Using SVMs on Banknote Authentication Data Set</h2>
<h3 id="data-preparation-and-active-learning-simulation">Data Preparation and Active Learning Simulation</h3>
<ul>
  <li><strong>Dataset Splitting</strong>: Randomly selecting a test and training set.</li>
  <li><strong>Passive Learning</strong>: Implementing passive learning by incrementally increasing the training set.</li>
  <li><strong>Active Learning</strong>: Employing active learning by selectively adding data points closest to the SVM hyperplane.</li>
</ul>

<h3 id="monte-carlo-simulation-and-analysis">Monte Carlo Simulation and Analysis</h3>
<ul>
  <li><strong>Simulation and Learning Curve</strong>: Conducting a Monte Carlo simulation to average the test errors for both active and passive learning. Plotting the learning curve and drawing conclusions from the results.</li>
</ul>

<p><a href="URL_to_your_GitHub_repository">View Project on GitHub</a></p>]]></content><author><name>Joshua Payapulli</name></author><category term="Data Science" /><summary type="html"><![CDATA[This project embarks on an exploratory journey through different realms of machine learning: supervised, semi-supervised, and unsupervised learning, using the Breast Cancer Wisconsin (Diagnostic) Data Set and the Banknote Authentication Data Set. The focus is on implementing various classification and clustering techniques, and comparing their effectiveness through a series of Monte-Carlo simulations.]]></summary></entry><entry><title type="html">Boosting, LASSO: Decision Trees and Advanced Regression Techniques on Crime data</title><link href="http://localhost:4000/data%20science/crime/" rel="alternate" type="text/html" title="Boosting, LASSO: Decision Trees and Advanced Regression Techniques on Crime data" /><published>2023-12-19T00:00:00-08:00</published><updated>2023-12-19T00:00:00-08:00</updated><id>http://localhost:4000/data%20science/crime</id><content type="html" xml:base="http://localhost:4000/data%20science/crime/"><![CDATA[<p>In this project, I delve into the realms of decision trees and advanced regression techniques, applying these methods to two distinct datasets: the Acute Inflammations dataset and the Communities and Crime dataset. The project is structured into two main sections, each focusing on different aspects of machine learning.</p>

<p><img src="/assets/images/crime-data.png" alt="Alt text for image" /></p>

<!--more-->

<h2 id="decision-trees-as-interpretable-models">Decision Trees as Interpretable Models</h2>
<h3 id="dataset-acquisition-and-tree-construction">Dataset Acquisition and Tree Construction</h3>
<ul>
  <li><strong>Data Download</strong>: Fetching the Acute Inflamations data from its official repository.</li>
  <li><strong>Decision Tree Modeling</strong>: Building a decision tree on the entire dataset and visualizing it.</li>
</ul>

<h3 id="rule-extraction-and-pruning">Rule Extraction and Pruning</h3>
<ul>
  <li><strong>Rule Conversion</strong>: Translating the decision rules of the tree into a set of IF-THEN rules for better interpretability.</li>
  <li><strong>Cost-Complexity Pruning</strong>: Implementing pruning techniques to find a minimal decision tree that maintains high interpretability.</li>
</ul>

<h2 id="the-lasso-and-boosting-for-regression">The LASSO and Boosting for Regression</h2>
<h3 id="data-handling-and-analysis">Data Handling and Analysis</h3>
<ul>
  <li><strong>Data Retrieval</strong>: Downloading the Communities and Crime dataset.</li>
  <li><strong>Data Preparation</strong>: Handling missing values and disregarding nonpredictive features.</li>
  <li><strong>Correlation Matrix</strong>: Creating a correlation matrix to understand feature relationships.</li>
  <li><strong>Coefficient of Variation Calculation</strong>: Computing the Coefficient of Variation (CV) for each feature.</li>
</ul>

<h3 id="feature-selection-and-visualization">Feature Selection and Visualization</h3>
<ul>
  <li><strong>Feature Shortlisting</strong>: Selecting a subset of features with the highest CV.</li>
  <li><strong>Data Visualization</strong>: Generating scatter plots and box plots for the shortlisted features to assess their significance.</li>
</ul>

<h3 id="regression-models-and-evaluation">Regression Models and Evaluation</h3>
<ul>
  <li><strong>Linear Regression</strong>: Fitting a linear model using least squares and reporting the test error.</li>
  <li><strong>Ridge Regression</strong>: Applying ridge regression with λ chosen by cross-validation and reporting the test error.</li>
  <li><strong>LASSO Regression</strong>: Conducting LASSO regression with λ chosen by cross-validation and comparing the test error between models with standard and non-standardized features.</li>
  <li><strong>PCR Model</strong>: Fitting a Principal Component Regression (PCR) model with the number of components chosen by cross-validation.</li>
</ul>

<h3 id="advanced-regression-with-boosting">Advanced Regression with Boosting</h3>
<ul>
  <li><strong>L1 Penalized Gradient Boosting Tree</strong>: Utilizing XGBoost to fit an L1 penalized gradient boosting tree, determining α through cross-validation.</li>
</ul>

<p><a href="URL_to_your_GitHub_repository">View Project on GitHub</a></p>]]></content><author><name>Joshua Payapulli</name></author><category term="Data Science" /><summary type="html"><![CDATA[In this project, I delve into the realms of decision trees and advanced regression techniques, applying these methods to two distinct datasets: the Acute Inflammations dataset and the Communities and Crime dataset. The project is structured into two main sections, each focusing on different aspects of machine learning.]]></summary></entry><entry><title type="html">Linear Regression, KNN Regression: Combined Cycle Power Plant Data</title><link href="http://localhost:4000/data%20science/ccpp/" rel="alternate" type="text/html" title="Linear Regression, KNN Regression: Combined Cycle Power Plant Data" /><published>2023-12-18T00:00:00-08:00</published><updated>2023-12-18T00:00:00-08:00</updated><id>http://localhost:4000/data%20science/ccpp</id><content type="html" xml:base="http://localhost:4000/data%20science/ccpp/"><![CDATA[<p>This project delves into a detailed dataset from a Combined Cycle Power Plant over six years (2006-2011), where the main goal is to predict the net hourly electrical energy output (EP). The dataset features include hourly average ambient variables: Temperature (T), Ambient Pressure (AP), Relative Humidity (RH), and Exhaust Vacuum (V)…</p>

<p><img src="/assets/images/Combined-Cycle-Power-Plant.jpeg" alt="Alt text for image" /></p>

<!--more-->

<h2 id="project-overview">Project Overview</h2>
<p>The project begins with downloading and exploring the dataset, examining the number of rows and columns, and understanding what they represent. This step includes making scatterplots of all variables and observing the relationships between independent and dependent variables.</p>

<h2 id="data-analysis-and-insights">Data Analysis and Insights</h2>
<p>Statistical analysis is conducted to find the mean, median, range, quartiles, and interquartile ranges of each variable. This is summarized in a comprehensive table for an easy overview of the data characteristics.</p>

<h2 id="regression-models">Regression Models</h2>
<p>The core of the project involves:</p>
<ul>
  <li>Developing simple linear regression models for each predictor and assessing their significance in predicting the plant’s output.</li>
  <li>Constructing a multiple regression model using all predictors and identifying the most impactful ones.</li>
  <li>A comparative study between univariate and multiple regression models is presented visually.</li>
</ul>

<h2 id="advanced-analysis">Advanced Analysis</h2>
<p>The exploration extends to:</p>
<ul>
  <li>Examining nonlinear associations between predictors and response using polynomial regression models.</li>
  <li>Investigating interactions between different predictors and their impact on the response.</li>
  <li>A thorough analysis of these models is presented, including the training and testing Mean Squared Errors (MSEs).</li>
</ul>

<h2 id="knn-regression">KNN Regression</h2>
<p>The project also includes KNN regression analysis, comparing both normalized and raw features to determine the best fit. The analysis of train and test errors provides valuable insights into the model’s performance.</p>

<h2 id="comparative-study-and-conclusions">Comparative Study and Conclusions</h2>
<p>The project concludes with a comparison between KNN Regression and the most effective linear regression model. This comparative analysis provides a deep understanding of different modeling approaches in predicting energy output in power plants.</p>

<p><a href="https://nbviewer.org/github/Payapulli/Payapulli.github.io/blob/main/jupyter-notebooks/Vertebral-Column-Datset-KNN.ipynb">View Jupyter Notebook</a> | 
<a href="URL_to_your_GitHub_repository">View Project on GitHub</a> |
<a href="https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant">View Dataset</a></p>]]></content><author><name>Joshua Payapulli</name></author><category term="Data Science" /><summary type="html"><![CDATA[This project delves into a detailed dataset from a Combined Cycle Power Plant over six years (2006-2011), where the main goal is to predict the net hourly electrical energy output (EP). The dataset features include hourly average ambient variables: Temperature (T), Ambient Pressure (AP), Relative Humidity (RH), and Exhaust Vacuum (V)…]]></summary></entry><entry><title type="html">KNN: Vertebral Column Data Set</title><link href="http://localhost:4000/data%20science/vertebral/" rel="alternate" type="text/html" title="KNN: Vertebral Column Data Set" /><published>2023-12-18T00:00:00-08:00</published><updated>2023-12-18T00:00:00-08:00</updated><id>http://localhost:4000/data%20science/vertebral</id><content type="html" xml:base="http://localhost:4000/data%20science/vertebral/"><![CDATA[<p>This project involves a comprehensive analysis of the Vertebral Column Data Set created by Dr. Henrique da Mota. The data set includes six biomechanical attributes from each patient, focusing on binary classification of Normal (NO) and Abnormal (AB) conditions…</p>

<p><img src="/assets/images/vertebral-column.jpeg" alt="Alt text for image" /></p>

<!--more-->

<h2 id="data-exploration">Data Exploration</h2>
<p>The dataset is explored through scatterplots and boxplots to understand the relationships and distributions of the variables.</p>

<h2 id="knn-classification">KNN Classification</h2>
<p>The project’s core is implementing K-Nearest Neighbors (KNN) with various metrics including Euclidean, Manhattan, and Chebyshev distances. Key points include:</p>
<ul>
  <li>Optimizing the value of <code class="language-plaintext highlighter-rouge">k</code> based on error rates.</li>
  <li>Calculating metrics like confusion matrix, true positive and negative rates, precision, and F1-score.</li>
  <li>Analyzing the learning curve against different training set sizes.</li>
</ul>

<h2 id="advanced-techniques">Advanced Techniques</h2>
<p>Further exploration includes:</p>
<ul>
  <li>Testing different distance metrics and summarizing their performance.</li>
  <li>Implementing weighted voting to reduce variance and improve decision making.</li>
</ul>

<h2 id="conclusions-and-findings">Conclusions and Findings</h2>
<p>We can see from our test errors using various distance metrics that the KNN model performs well on this dataset with a test error rate averaging at around 0.1.</p>

<p><a href="https://nbviewer.org/github/Payapulli/Payapulli.github.io/blob/main/jupyter-notebooks/Vertebral-Column-Datset-KNN.ipynb">View Jupyter Notebook</a> |
<a href="URL_to_your_GitHub_repository">View Project on GitHub</a> |
<a href="https://archive.ics.uci.edu/dataset/212/vertebral+column">View Dataset</a></p>]]></content><author><name>Joshua Payapulli</name></author><category term="Data Science" /><summary type="html"><![CDATA[This project involves a comprehensive analysis of the Vertebral Column Data Set created by Dr. Henrique da Mota. The data set includes six biomechanical attributes from each patient, focusing on binary classification of Normal (NO) and Abnormal (AB) conditions…]]></summary></entry></feed>